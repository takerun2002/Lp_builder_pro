# 調査結果サマリー

## 1. 最強Pythonリサーチスクレイパー

### 推奨ツール: GPT Researcher

- **GitHub URL**: [https://github.com/assafelovic/gpt-researcher](https://github.com/assafelovic/gpt-researcher)
- **総合評価**: ⭐⭐⭐⭐⭐ (5/5)

**推奨理由**:

GPT Researcherは、リサーチタスクに特化した最も包括的なオープンソースソリューションです。Deep Research機能、20以上の多様なソースからの情報集約能力、そして引用元の追跡機能を標準で備えており、学術・ビジネス両方の高度な要求に応えます。MCP（Model Context Protocol）への対応により、外部ツールとの連携や拡張が容易であり、将来性も非常に高いと評価できます。さらに、セルフホストが可能で、活発なコミュニティと充実したドキュメントが存在するため、導入と運用の両面で安心感があります。

**短所・注意点**:

一方で、LangChainやFastAPIなど多くのライブラリに依存しているため、セットアップがやや複雑になる可能性があります。また、多機能である分、小規模なタスクにはリソース消費が過剰となる場合があります。

### その他の有力候補

| プロジェクト名 | GitHubスター数 | 主要機能 | 総合評価 | 短評 |
| :--- | :--- | :--- | :--- | :--- |
| **Firecrawl** | 69.7k | 高速なクローリング、LLM対応マークダウン変換 | ⭐⭐⭐⭐⭐ | スクレイピング性能は最高クラス。API中心のため、手軽に導入可能だが、リサーチ特化ではない。 |
| **Open Deep Research** | 9.9k | LangChain公式、Deep Research Bench評価済み | ⭐⭐⭐⭐ | 高度にカスタマイズ可能で、LangChainエコシステムとの親和性が高い。比較的新しいプロジェクト。 |
| **ScrapeGraphAI** | 22k | LLMベースのグラフロジックによるスクレイピング | ⭐⭐⭐⭐ | プロンプトだけでスクレイピングパイプラインを自動生成できる点がユニーク。LLMの性能に依存する。 |
| **Scrapy** | 59.2k | 高速・高機能なスクレイピングフレームワーク | ⭐⭐⭐⭐ | AI機能はないが、大規模クローリングの基盤技術として非常に強力。他のツールとの組み合わせが前提。 |

---

## 2. Gemini Interactions API統合ツール

### API概要

Googleが2025年12月に発表した**Gemini Interactions API**は、Geminiモデルと専用エージェントを操作するための新しい統一インターフェースです。このAPIの最大の特徴は、**Gemini Deep Research Agent**という自律型リサーチエージェントを呼び出せる点にあります。このエージェントは、複雑なリサーチタスクを自律的に計画・実行し、引用文献付きの詳細なレポートを生成します。非同期でのバックグラウンド実行が推奨されており、数分を要する長時間タスクに対応しています。

### 既存統合ツール

現時点で、Gemini Interactions APIを直接統合したサードパーティ製の完成されたリサーチツールはまだ登場していません。しかし、以下の公式リソースや競合の動向が参考になります。

| リソース/ツール | 概要 | リンク |
| :--- | :--- | :--- |
| **Google Gemini Cookbook** | Interactions APIやDeep Researchの実装例を含む公式コード集。 | [GitHub](https://github.com/google-gemini/cookbook) |
| **Google AI Studio** | APIを無料でテストできるWeb UI。プロトタイピングに最適。 | [Webサイト](https://aistudio.google.com) |
| **Perplexity Deep Research** | 競合となるAI検索エンジン。API経由で同様の機能を提供。 | [Webサイト](https://www.perplexity.ai/api-platform) |
| **Anthropic Claude Research** | 競合LLM。Tool Search機能やMulti-agentシステムを持つ。 | [Webサイト](https://www.anthropic.com/engineering/multi-agent-research-system) |

### 統合アーキテクチャ提案

PythonリサーチスクレイパーとGemini Interactions APIを統合するための推奨アーキテクチャは、**ハイブリッド型リサーチエージェント**です。このモデルでは、役割の異なる2つのコンポーネントが連携します。

1.  **高速スクレイパー (Firecrawl/ScrapeGraphAI)**: 初期情報収集を担当。特定のWebサイトや構造化されたデータを迅速に取得し、キャッシュします。
2.  **Deep Researchエージェント (Gemini)**: スクレイパーが収集した情報をコンテキストとして与え、さらにWeb全体から情報を収集・分析・統合し、最終的なレポートを生成します。

このアーキテクチャにより、高速性と網羅性の両立、そしてコスト効率の最適化が期待できます。

```mermaid
graph TD
    A[ユーザー入力] --> B{リサーチオーケストレーター};
    B --> C[フェーズ1: 高速スクレイピング<br>(Firecrawl)];
    B --> D[フェーズ2: Deep Research<br>(Gemini Interactions API)];
    C --> D;
    D --> E[フェーズ3: レポート生成];
    E --> F[最終レポート];
```

### 実装ガイドライン

#### ステップ1: 環境構築

まず、必要なライブラリをインストールし、APIキーを環境変数に設定します。

```bash
pip install google-generativeai firecrawl-py
export GEMINI_API_KEY="YOUR_API_KEY"
export FIRECRAWL_API_KEY="YOUR_FIRECRAWL_KEY"
```

#### ステップ2: ハイブリッドエージェントの実装

Firecrawlで初期情報を収集し、その結果をコンテキストとしてGemini Deep Research Agentに渡すことで、より精度の高いリサーチを実行します。API呼び出しは非同期で行い、長時間実行に対応します。

```python
import google.generativeai as genai
from firecrawl import FirecrawlApp
import asyncio

class HybridResearchAgent:
    def __init__(self, gemini_key, firecrawl_key):
        genai.configure(api_key=gemini_key)
        self.fc = FirecrawlApp(api_key=firecrawl_key)
        self.client = genai.Client()

    async def research(self, query: str):
        # Firecrawlで初期データ収集
        scraped_data = self.fc.scrape_url(f"https://www.google.com/search?q={query}")['markdown']
        
        # Gemini Deep Researchで詳細分析
        prompt = f"""
        以下の初期コンテキストを参考に、'{query}'について詳細なリサーチレポートを作成してください。

        初期コンテキスト:
        {scraped_data}
        """
        
        response = self.client.interactions.create(
            model="deep-research-pro-preview-12-2025",
            input=prompt,
            background=True
        )
        
        # 結果をポーリング
        while True:
            status = self.client.interactions.get(response.id)
            if status.status == "completed":
                return status.outputs[0].text
            elif status.status == "failed":
                raise Exception("Deep Research failed")
            await asyncio.sleep(10)
```

#### ステップ3: コストとリスクの管理

- **コスト**: Deep Researchは1レポートあたり約$0.20〜$1.00のコストが見込まれます。クエリ結果をキャッシュすることで、コストを30〜50%削減可能です。
- **レート制限**: APIには無料枠で15リクエスト/分、有料枠で600リクエスト/分の制限があります。指数バックオフを伴うリトライ処理の実装が不可欠です。
- **APIの変更**: Interactions APIは現在ベータ版であり、将来的に互換性を破る変更が含まれる可能性があります。バージョンを固定するなどの対策を推奨します。


---

## 3. 統合アーキテクチャと実装ガイドライン（詳細）

### 推奨アーキテクチャ: ハイブリッド型リサーチエージェント

本プロジェクトで推奨するアーキテクチャは、**Firecrawl** の高速スクレイピング能力と **Gemini Deep Research Agent** の高度な分析能力を組み合わせたハイブリッドモデルです。この設計により、速度、コスト、品質のバランスを最適化します。

#### アーキテクチャ図

```mermaid
graph TD
    subgraph User Interface
        A[Web/CLI] --> B{Orchestrator};
    end

    subgraph Orchestrator Layer
        B -- 1. Query --> C[Task Planner];
        C -- 2. Decompose --> D{Agent Executor};
    end

    subgraph Agent Layer
        D -- 3. Execute --> E(Firecrawl Agent);
        D -- 3. Execute --> F(Gemini Agent);
    end

    subgraph Tool & Data Layer
        E -- 4. Scrape --> G[Websites/Documents];
        F -- 5. Deep Research --> H[Google Search & Context];
        G --> I[Cache (Redis)];
        H --> I;
    end

    subgraph Synthesis Layer
        I -- 6. Aggregate --> J[Report Synthesizer];
        J -- 7. Generate --> K[Final Report];
    end

    B --> J;
```

#### ワークフロー解説

1.  **クエリ入力**: ユーザーがリサーチしたいトピックを入力します。
2.  **タスク分解**: オーケストレーター（例: LangChain）がクエリを分析し、初期データ収集と詳細分析の2つのサブタスクに分解します。
3.  **並列実行**: Firecrawlエージェントが指定されたURLや検索結果から高速に情報を収集し、LLMが処理しやすいようにマークダウン形式に変換します。同時に、GeminiエージェントがDeep Researchを開始します。
4.  **データ収集**: Firecrawlはウェブサイトから直接データを取得します。
5.  **Deep Research**: GeminiはGoogle検索と、Firecrawlが収集したコンテキスト情報を利用して、多角的な分析を行います。
6.  **結果集約**: 両エージェントの出力とキャッシュされたデータが統合されます。
7.  **レポート生成**: 最終的なレポートが生成され、ユーザーに提示されます。

### 実装ガイドライン（詳細版）

#### 1. エージェントの状態管理

エージェントの複雑な状態遷移を管理するために、`LangGraph` のような状態マシンベースのフレームワークの導入を強く推奨します。これにより、リトライ、エラーハンドリング、条件分岐などを宣言的に記述できます。

```python
from langgraph.graph import StateGraph, START, END
from typing import TypedDict, List

class ResearchState(TypedDict):
    query: str
    scraped_data: str
    research_summary: str
    final_report: str
    error: str

# ... (各ノードの関数定義) ...

workflow = StateGraph(ResearchState)
workflow.add_node("scrape", scrape_node)
workflow.add_node("research", research_node)
workflow.add_node("synthesize", synthesize_node)

workflow.add_edge(START, "scrape")
workflow.add_conditional_edges(
    "scrape",
    lambda state: "research" if not state.get("error") else END,
    {"research": "research", "__end__": END}
)
# ...
```

#### 2. エラーハンドリングと堅牢性

-   **APIエラー**: `tenacity`ライブラリを使用して、指数バックオフ付きのリトライ処理を実装します。
-   **タイムアウト**: `asyncio.wait_for` を使用して、各API呼び出しにタイムアウトを設定します。
-   **データ品質**: スクレイピング結果が不十分な場合や、Deep Researchが低品質なレポートを生成した場合は、フォールバックとして別の検索ツール（例: Tavily）を呼び出す、あるいはユーザーに再確認を促すなどの処理を実装します。

#### 3. コスト管理と最適化

-   **キャッシュ**: Redisやインメモリキャッシュを使用して、同一のURLやクエリに対するAPI呼び出しを削減します。キャッシュのキーには、URLやクエリのハッシュ値を使用します。
-   **モデル選択**: 初期分析や要約には、より高速で安価なモデル（例: `gemini-1.5-flash`）を使用し、最終的なレポート生成にのみ高性能なモデル（例: `gemini-1.5-pro`）を使用することで、コストを最適化します。
-   **予算アラート**: 月間のAPI利用額が設定した閾値を超えた場合に、管理者に通知する仕組みを導入します。

#### 4. セキュリティ

-   **APIキー管理**: APIキーは環境変数や、HashiCorp Vault、AWS Secrets Managerなどのシークレット管理サービスを使用して安全に管理します。
-   **入力サニタイズ**: ユーザーからの入力に含まれる可能性のある悪意のあるコードやプロンプトインジェクション攻撃を緩和するため、入力をサニタイズする処理を追加します。

### リスク評価と対策

| リスクカテゴリ | 具体的なリスク | 影響度 | 発生確率 | 対策 |
| :--- | :--- | :--- | :--- | :--- |
| **技術的リスク** | APIの仕様変更（ベータ版） | 高 | 中 | - APIバージョンの固定<br>- 定期的なドキュメント確認とテスト |
| | 依存ライブラリの脆弱性 | 中 | 低 | - `pip-audit` などによる定期的な脆弱性スキャン<br>- 依存関係の定期的な更新 |
| **コストリスク** | 想定外のAPI利用によるコスト超過 | 高 | 中 | - 予算アラートの設定<br>- 詳細な利用状況のモニタリング<br>- 厳格なキャッシュ戦略 |
| **品質リスク** | 生成されるレポートの品質が低い | 中 | 中 | - 複数ソースからのクロスチェック<br>- 生成結果に対する評価プロンプトの導入<br>- ユーザーからのフィードバックループ構築 |
| **運用リスク** | レート制限によるサービス停止 | 中 | 高 | - 指数バックオフによるリトライ<br>- 複数APIキーによる負荷分散<br>- 有料プランへの移行検討 |

---

## 4. 参考文献

1.  [GPT Researcher - GitHub](https://github.com/assafelovic/gpt-researcher)
2.  [Firecrawl - GitHub](https://github.com/firecrawl/firecrawl)
3.  [Open Deep Research - GitHub](https://github.com/langchain-ai/open_deep_research)
4.  [Gemini API Documentation](https://ai.google.dev/gemini-api/docs)
5.  [Gemini Interactions API](https://ai.google.dev/gemini-api/docs/interactions)
6.  [Google Gemini Fullstack LangGraph Quickstart](https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart)
7.  [LangChain Documentation](https://docs.langchain.com/)
