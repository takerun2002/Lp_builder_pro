# Claude Code æŒ‡ç¤ºæ›¸: Crawl4AI çµ±åˆ

## æ¦‚è¦

LP Builder Proã«Crawl4AIï¼ˆé«˜ç²¾åº¦LLMãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼‰ã‚’çµ±åˆã—ã€LPã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ç­‰ã‹ã‚‰ã®ãƒ‡ã‚¶ã‚¤ãƒ³ãƒªã‚µãƒ¼ãƒæ©Ÿèƒ½ã‚’å¼·åŒ–ã™ã‚‹ã€‚

## èƒŒæ™¯

- ç¾åœ¨ã®Playwrightãƒ™ãƒ¼ã‚¹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¯ç²¾åº¦ãŒä½ã„
- LPã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼ˆrdlp.jp/lp-archive/ï¼‰ã¯å‹•çš„ã‚µã‚¤ãƒˆã§ãƒœãƒƒãƒˆæ¤œå‡ºãŒã‚ã‚‹
- Crawl4AIã¯ãƒœãƒƒãƒˆæ¤œå‡ºå›é¿ã€LLMæ§‹é€ åŒ–æŠ½å‡ºã‚’ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚µãƒãƒ¼ãƒˆ
- 46k+ GitHub Starsã®äººæ°—ãƒ„ãƒ¼ãƒ«

## Crawl4AIã«ã¤ã„ã¦

GitHub: https://github.com/unclecode/crawl4ai

### ä¸»ãªç‰¹å¾´
- ãƒœãƒƒãƒˆæ¤œå‡ºå›é¿æ©Ÿèƒ½å†…è”µ
- LLMã§æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆGeminiå¯¾å¿œï¼‰
- Playwrightä¸Šã«æ§‹ç¯‰ï¼ˆç²¾åº¦å‘ä¸Šï¼‰
- Managed Browserå¯¾å¿œ
- å®Œå…¨ç„¡æ–™ï¼ˆPythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰

---

## å®Ÿè£…ã‚¿ã‚¹ã‚¯

### Phase 1: Pythonç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

#### 1.1 Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ

```
/python-scripts/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ crawl4ai_server.py      # FastAPIã‚µãƒ¼ãƒãƒ¼
â”œâ”€â”€ lp_archive_scraper.py   # LPã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å°‚ç”¨
â””â”€â”€ design_gallery_scraper.py # æ±ç”¨ãƒ‡ã‚¶ã‚¤ãƒ³ã‚®ãƒ£ãƒ©ãƒªãƒ¼
```

#### 1.2 requirements.txt

```txt
crawl4ai>=0.4.0
fastapi>=0.109.0
uvicorn>=0.27.0
pydantic>=2.0.0
google-generativeai>=0.3.0
```

#### 1.3 FastAPI ã‚µãƒ¼ãƒãƒ¼ï¼ˆcrawl4ai_server.pyï¼‰

```python
"""
Crawl4AI ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚µãƒ¼ãƒãƒ¼
LP Builder Pro ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹
"""
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional, List
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy, JsonCssExtractionStrategy
import os

app = FastAPI(title="LP Builder Pro - Crawl4AI Server")

class ScrapeRequest(BaseModel):
    url: str
    image_type: Optional[str] = None  # é«˜ç´šãƒ»ã‚»ãƒ¬ãƒ–, ã‚·ãƒ³ãƒ—ãƒ«, etc.
    color: Optional[str] = None
    limit: int = 10
    use_llm: bool = True
    gemini_api_key: Optional[str] = None

class LPResult(BaseModel):
    title: str
    thumbnail_url: str
    lp_url: str
    category: Optional[str] = None

class ScrapeResponse(BaseModel):
    success: bool
    results: List[LPResult]
    error: Optional[str] = None

@app.post("/scrape/lp-archive", response_model=ScrapeResponse)
async def scrape_lp_archive(request: ScrapeRequest):
    """LPã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    try:
        async with AsyncWebCrawler(
            headless=True,
            browser_type="chromium",
            verbose=False
        ) as crawler:
            # ã‚«ãƒ†ã‚´ãƒªé¸æŠç”¨ã®URLæ§‹ç¯‰
            url = request.url
            
            # LLMæŠ½å‡ºæˆ¦ç•¥ã‚’è¨­å®š
            if request.use_llm and request.gemini_api_key:
                extraction_strategy = LLMExtractionStrategy(
                    provider=f"gemini/gemini-2.0-flash",
                    api_token=request.gemini_api_key,
                    extraction_type="schema",
                    schema={
                        "type": "object",
                        "properties": {
                            "lps": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "properties": {
                                        "title": {"type": "string"},
                                        "thumbnail_url": {"type": "string"},
                                        "lp_url": {"type": "string"},
                                        "category": {"type": "string"}
                                    }
                                }
                            }
                        }
                    },
                    instruction=f"""
                    ã“ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰LPï¼ˆãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒšãƒ¼ã‚¸ï¼‰ã®ä¸€è¦§ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
                    å„LPã«ã¤ã„ã¦ä»¥ä¸‹ã‚’å–å¾—:
                    - title: LPã®ã‚¿ã‚¤ãƒˆãƒ«
                    - thumbnail_url: ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒã®URL
                    - lp_url: LPã¸ã®ãƒªãƒ³ã‚¯
                    - category: ã‚«ãƒ†ã‚´ãƒªï¼ˆã‚ã‚Œã°ï¼‰
                    æœ€å¤§{request.limit}ä»¶ã¾ã§å–å¾—ã—ã¦ãã ã•ã„ã€‚
                    """
                )
            else:
                # CSS ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼æˆ¦ç•¥ï¼ˆLLMä¸ä½¿ç”¨ï¼‰
                extraction_strategy = JsonCssExtractionStrategy(
                    schema={
                        "name": "LP List",
                        "baseSelector": ".lp-item, [class*='lp-card'], article",
                        "fields": [
                            {"name": "title", "selector": "h3, .title, [class*='title']", "type": "text"},
                            {"name": "thumbnail_url", "selector": "img", "type": "attribute", "attribute": "src"},
                            {"name": "lp_url", "selector": "a", "type": "attribute", "attribute": "href"}
                        ]
                    }
                )
            
            config = CrawlerRunConfig(
                extraction_strategy=extraction_strategy,
                wait_for="css:.lp-item, css:[class*='lp-card'], css:article",
                delay_before_return_html=2.0,
                screenshot=False,
                anti_detection=True  # ãƒœãƒƒãƒˆæ¤œå‡ºå›é¿
            )
            
            result = await crawler.arun(url=url, config=config)
            
            if result.success:
                extracted = result.extracted_content
                # ãƒ‘ãƒ¼ã‚¹å‡¦ç†
                lps = []
                if isinstance(extracted, dict) and "lps" in extracted:
                    lps = extracted["lps"][:request.limit]
                elif isinstance(extracted, list):
                    lps = extracted[:request.limit]
                
                return ScrapeResponse(
                    success=True,
                    results=[LPResult(**lp) for lp in lps if lp.get("lp_url")]
                )
            else:
                return ScrapeResponse(
                    success=False,
                    results=[],
                    error=result.error_message
                )
                
    except Exception as e:
        return ScrapeResponse(
            success=False,
            results=[],
            error=str(e)
        )

@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "crawl4ai-server"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8765)
```

---

### Phase 2: Next.js APIé€£æº

#### 2.1 Crawl4AI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

**ãƒ•ã‚¡ã‚¤ãƒ«: `src/lib/scrapers/crawl4ai-client.ts`**

```typescript
/**
 * Crawl4AI Python ã‚µãƒ¼ãƒãƒ¼ã¨ã®é€£æºã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
 */

export interface LPResult {
  title: string;
  thumbnail_url: string;
  lp_url: string;
  category?: string;
}

export interface ScrapeRequest {
  url: string;
  imageType?: string;
  color?: string;
  limit?: number;
  useLlm?: boolean;
  geminiApiKey?: string;
}

export interface ScrapeResponse {
  success: boolean;
  results: LPResult[];
  error?: string;
}

const CRAWL4AI_SERVER_URL = process.env.CRAWL4AI_SERVER_URL || "http://localhost:8765";

export async function scrapeLPArchive(request: ScrapeRequest): Promise<ScrapeResponse> {
  try {
    const response = await fetch(`${CRAWL4AI_SERVER_URL}/scrape/lp-archive`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        url: request.url,
        image_type: request.imageType,
        color: request.color,
        limit: request.limit || 10,
        use_llm: request.useLlm ?? true,
        gemini_api_key: request.geminiApiKey,
      }),
    });

    if (!response.ok) {
      throw new Error(`Crawl4AI server error: ${response.status}`);
    }

    return await response.json();
  } catch (error) {
    console.error("[crawl4ai-client] Error:", error);
    return {
      success: false,
      results: [],
      error: error instanceof Error ? error.message : "Unknown error",
    };
  }
}

export async function checkCrawl4AIHealth(): Promise<boolean> {
  try {
    const response = await fetch(`${CRAWL4AI_SERVER_URL}/health`);
    return response.ok;
  } catch {
    return false;
  }
}
```

#### 2.2 API Route

**ãƒ•ã‚¡ã‚¤ãƒ«: `src/app/api/scrape/lp-archive/route.ts`**

```typescript
import { NextRequest, NextResponse } from "next/server";
import { scrapeLPArchive, checkCrawl4AIHealth } from "@/lib/scrapers/crawl4ai-client";
import { getApiKey } from "@/lib/api-keys";

export async function POST(request: NextRequest) {
  try {
    const body = await request.json();
    const { imageType, color, limit = 10 } = body;

    // Crawl4AIã‚µãƒ¼ãƒãƒ¼ã®ç¨¼åƒç¢ºèª
    const isHealthy = await checkCrawl4AIHealth();
    if (!isHealthy) {
      return NextResponse.json(
        { error: "Crawl4AIã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“ã€‚python-scripts/crawl4ai_server.py ã‚’èµ·å‹•ã—ã¦ãã ã•ã„ã€‚" },
        { status: 503 }
      );
    }

    // Gemini APIã‚­ãƒ¼ã‚’å–å¾—
    const geminiApiKey = process.env.GEMINI_API_KEY || await getApiKey("gemini");

    const result = await scrapeLPArchive({
      url: "https://rdlp.jp/lp-archive/",
      imageType,
      color,
      limit,
      useLlm: !!geminiApiKey,
      geminiApiKey,
    });

    return NextResponse.json(result);
  } catch (error) {
    console.error("[lp-archive] API Error:", error);
    return NextResponse.json(
      { error: error instanceof Error ? error.message : "ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã«å¤±æ•—ã—ã¾ã—ãŸ" },
      { status: 500 }
    );
  }
}

// ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ç”¨
export async function GET() {
  const isHealthy = await checkCrawl4AIHealth();
  return NextResponse.json({
    crawl4ai: isHealthy ? "running" : "stopped",
    message: isHealthy ? "Crawl4AIã‚µãƒ¼ãƒãƒ¼ç¨¼åƒä¸­" : "python-scripts/crawl4ai_server.py ã‚’èµ·å‹•ã—ã¦ãã ã•ã„"
  });
}
```

---

### Phase 3: ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰UI

#### 3.1 ãƒ‡ã‚¶ã‚¤ãƒ³ãƒªã‚µãƒ¼ãƒãƒšãƒ¼ã‚¸

**ãƒ•ã‚¡ã‚¤ãƒ«: `src/app/dev/design-research/page.tsx`**

```tsx
"use client";

import { useState } from "react";
import { Button } from "@/components/ui/button";
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card";
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from "@/components/ui/select";
import { Loader2, Search, ExternalLink, Plus } from "lucide-react";

const IMAGE_TYPES = [
  "ãƒãƒ³ã‚¬ãƒ»ã‚¤ãƒ©ã‚¹ãƒˆ",
  "å’Œé¢¨",
  "ã‹ã£ã“ã„ã„",
  "ã‹ã‚ã„ã„",
  "ã«ãã‚„ã‹",
  "ã‚·ãƒ³ãƒ—ãƒ«",
  "ã‚¢ãƒ¼ãƒˆãƒ»èŠ¸è¡“",
  "ã‚­ãƒ¬ã‚¤",
  "ä¿¡é ¼ãƒ»å®‰å¿ƒ",
  "å¥åº·ãƒ»ç™’ã—",
  "åŠ›å¼·ã„",
  "æ´¾æ‰‹",
  "æ¸…æ½”",
  "çˆ½ã‚„ã‹",
  "ç¥ç§˜",
  "é«˜ç´šãƒ»ã‚»ãƒ¬ãƒ–",
  "ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯",
  "ãƒŠãƒãƒ¥ãƒ©ãƒ«",
  "ã‚¹ã‚¿ã‚¤ãƒªãƒƒã‚·ãƒ¥",
  "é€æ˜æ„Ÿ",
];

interface LPResult {
  title: string;
  thumbnail_url: string;
  lp_url: string;
  category?: string;
}

export default function DesignResearchPage() {
  const [imageType, setImageType] = useState<string>("");
  const [results, setResults] = useState<LPResult[]>([]);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [serverStatus, setServerStatus] = useState<"unknown" | "running" | "stopped">("unknown");

  const checkServerStatus = async () => {
    try {
      const res = await fetch("/api/scrape/lp-archive");
      const data = await res.json();
      setServerStatus(data.crawl4ai === "running" ? "running" : "stopped");
    } catch {
      setServerStatus("stopped");
    }
  };

  const handleSearch = async () => {
    setLoading(true);
    setError(null);

    try {
      const res = await fetch("/api/scrape/lp-archive", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          imageType,
          limit: 20,
        }),
      });

      const data = await res.json();

      if (data.success) {
        setResults(data.results);
      } else {
        setError(data.error || "æ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸ");
      }
    } catch (err) {
      setError(err instanceof Error ? err.message : "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ");
    } finally {
      setLoading(false);
    }
  };

  const addToSwipeFile = async (lp: LPResult) => {
    // TODO: ã‚¹ãƒ¯ã‚¤ãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½åŠ ã™ã‚‹å‡¦ç†
    console.log("Add to swipe file:", lp);
  };

  return (
    <div className="container max-w-6xl mx-auto py-8 px-4">
      <div className="flex items-center justify-between mb-8">
        <div>
          <h1 className="text-2xl font-bold">ğŸ¨ ãƒ‡ã‚¶ã‚¤ãƒ³ãƒªã‚µãƒ¼ãƒ</h1>
          <p className="text-muted-foreground mt-1">
            LPã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‹ã‚‰ãƒˆãƒ³ãƒãƒŠå‚è€ƒã‚’æ¤œç´¢
          </p>
        </div>
        <Button variant="outline" onClick={checkServerStatus}>
          ã‚µãƒ¼ãƒãƒ¼çŠ¶æ…‹ç¢ºèª
        </Button>
      </div>

      {serverStatus === "stopped" && (
        <Card className="mb-6 border-yellow-500 bg-yellow-50">
          <CardContent className="p-4">
            <p className="text-sm text-yellow-800">
              âš ï¸ Crawl4AIã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§èµ·å‹•ã—ã¦ãã ã•ã„:
            </p>
            <pre className="mt-2 p-2 bg-yellow-100 rounded text-xs">
              cd python-scripts && pip install -r requirements.txt && python crawl4ai_server.py
            </pre>
          </CardContent>
        </Card>
      )}

      {/* æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ  */}
      <Card className="mb-6">
        <CardHeader>
          <CardTitle className="text-lg">æ¤œç´¢æ¡ä»¶</CardTitle>
        </CardHeader>
        <CardContent className="space-y-4">
          <div className="flex gap-4">
            <div className="flex-1">
              <label className="text-sm font-medium mb-2 block">ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—</label>
              <Select value={imageType} onValueChange={setImageType}>
                <SelectTrigger>
                  <SelectValue placeholder="é¸æŠã—ã¦ãã ã•ã„" />
                </SelectTrigger>
                <SelectContent>
                  {IMAGE_TYPES.map((type) => (
                    <SelectItem key={type} value={type}>
                      {type}
                    </SelectItem>
                  ))}
                </SelectContent>
              </Select>
            </div>
            <div className="flex items-end">
              <Button onClick={handleSearch} disabled={loading}>
                {loading ? (
                  <>
                    <Loader2 className="w-4 h-4 mr-2 animate-spin" />
                    æ¤œç´¢ä¸­...
                  </>
                ) : (
                  <>
                    <Search className="w-4 h-4 mr-2" />
                    æ¤œç´¢
                  </>
                )}
              </Button>
            </div>
          </div>
        </CardContent>
      </Card>

      {/* ã‚¨ãƒ©ãƒ¼è¡¨ç¤º */}
      {error && (
        <Card className="mb-6 border-red-500 bg-red-50">
          <CardContent className="p-4">
            <p className="text-sm text-red-800">âŒ {error}</p>
          </CardContent>
        </Card>
      )}

      {/* æ¤œç´¢çµæœ */}
      {results.length > 0 && (
        <div>
          <h2 className="text-lg font-semibold mb-4">
            æ¤œç´¢çµæœ ({results.length}ä»¶)
          </h2>
          <div className="grid grid-cols-2 md:grid-cols-3 lg:grid-cols-4 gap-4">
            {results.map((lp, index) => (
              <Card key={index} className="overflow-hidden hover:shadow-lg transition-shadow">
                <div className="aspect-[3/4] relative bg-muted">
                  {lp.thumbnail_url && (
                    <img
                      src={lp.thumbnail_url}
                      alt={lp.title}
                      className="w-full h-full object-cover"
                      loading="lazy"
                    />
                  )}
                </div>
                <CardContent className="p-3">
                  <h3 className="text-sm font-medium line-clamp-2 mb-2">
                    {lp.title || "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"}
                  </h3>
                  <div className="flex gap-2">
                    <Button
                      size="sm"
                      variant="outline"
                      className="flex-1"
                      onClick={() => window.open(lp.lp_url, "_blank")}
                    >
                      <ExternalLink className="w-3 h-3 mr-1" />
                      é–‹ã
                    </Button>
                    <Button
                      size="sm"
                      className="flex-1"
                      onClick={() => addToSwipeFile(lp)}
                    >
                      <Plus className="w-3 h-3 mr-1" />
                      è¿½åŠ 
                    </Button>
                  </div>
                </CardContent>
              </Card>
            ))}
          </div>
        </div>
      )}
    </div>
  );
}
```

---

### Phase 4: ã‚µã‚¤ãƒ‰ãƒãƒ¼è¿½åŠ 

#### 4.1 ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ãƒ‡ã‚¶ã‚¤ãƒ³ãƒªã‚µãƒ¼ãƒã‚’è¿½åŠ 

**ãƒ•ã‚¡ã‚¤ãƒ«: `src/components/sidebar.tsx` ã«è¿½åŠ **

```tsx
// ãƒ„ãƒ¼ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«è¿½åŠ 
{
  name: "ãƒ‡ã‚¶ã‚¤ãƒ³ãƒªã‚µãƒ¼ãƒ",
  href: "/dev/design-research",
  icon: Palette,
  description: "LPã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‹ã‚‰ãƒˆãƒ³ãƒãƒŠå‚è€ƒã‚’æ¤œç´¢",
}
```

---

### Phase 5: å°†æ¥æ‹¡å¼µï¼ˆSkyvernçµ±åˆæº–å‚™ï¼‰

#### 5.1 ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼æŠ½è±¡åŒ–

**ãƒ•ã‚¡ã‚¤ãƒ«: `src/lib/scrapers/types.ts`**

```typescript
export type ScraperEngine = "playwright" | "crawl4ai" | "skyvern";

export interface ScraperConfig {
  engine: ScraperEngine;
  url: string;
  options?: Record<string, unknown>;
}

export interface ScraperResult {
  success: boolean;
  data: unknown;
  error?: string;
  engine: ScraperEngine;
}

export interface ScraperProvider {
  name: ScraperEngine;
  scrape(config: ScraperConfig): Promise<ScraperResult>;
  isAvailable(): Promise<boolean>;
}
```

---

## ç’°å¢ƒå¤‰æ•°

`.env.local` ã«è¿½åŠ :

```env
# Crawl4AI Python ã‚µãƒ¼ãƒãƒ¼URL
CRAWL4AI_SERVER_URL=http://localhost:8765
```

---

## èµ·å‹•æ‰‹é †

### 1. Pythonç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
cd python-scripts
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Crawl4AIã‚µãƒ¼ãƒãƒ¼èµ·å‹•

```bash
cd python-scripts
python crawl4ai_server.py
# ã‚µãƒ¼ãƒãƒ¼ãŒ http://localhost:8765 ã§èµ·å‹•
```

### 3. Next.jsé–‹ç™ºã‚µãƒ¼ãƒãƒ¼èµ·å‹•

```bash
npm run dev
```

### 4. ã‚¢ã‚¯ã‚»ã‚¹

- ãƒ‡ã‚¶ã‚¤ãƒ³ãƒªã‚µãƒ¼ãƒãƒšãƒ¼ã‚¸: http://localhost:3000/dev/design-research

---

## å®Œäº†æ¡ä»¶

### Phase 1
- [ ] python-scripts/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
- [ ] requirements.txt ä½œæˆ
- [ ] crawl4ai_server.py ä½œæˆãƒ»å‹•ä½œç¢ºèª

### Phase 2
- [ ] crawl4ai-client.ts ä½œæˆ
- [ ] /api/scrape/lp-archive API Route ä½œæˆ
- [ ] ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½å‹•ä½œç¢ºèª

### Phase 3
- [ ] /dev/design-research ãƒšãƒ¼ã‚¸ä½œæˆ
- [ ] æ¤œç´¢æ©Ÿèƒ½å‹•ä½œç¢ºèª
- [ ] ã‚¹ãƒ¯ã‚¤ãƒ—ãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ æ©Ÿèƒ½

### Phase 4
- [ ] ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ãƒªãƒ³ã‚¯è¿½åŠ 

### Phase 5ï¼ˆå°†æ¥ï¼‰
- [ ] Skyvernçµ±åˆæº–å‚™
- [ ] ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼æŠ½è±¡åŒ–

---

## æ³¨æ„äº‹é …

1. **Pythonã‚µãƒ¼ãƒãƒ¼ã¯åˆ¥ãƒ—ãƒ­ã‚»ã‚¹**
   - Next.jsã¨ã¯åˆ¥ã«Pythonã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
   - å°†æ¥çš„ã«ã¯Docker Composeã§çµ±åˆå¯èƒ½

2. **LLMä½¿ç”¨æ™‚ã®ã‚³ã‚¹ãƒˆ**
   - Gemini 2.0 Flash ã¯ç„¡æ–™æ ã‚ã‚Š
   - å¤§é‡ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ™‚ã¯LLMä¸ä½¿ç”¨ãƒ¢ãƒ¼ãƒ‰ã‚‚ç”¨æ„

3. **ãƒ¬ãƒ¼ãƒˆåˆ¶é™**
   - LPã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã¸ã®éåº¦ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é¿ã‘ã‚‹
   - é©åˆ‡ãªé…å»¶ã‚’è¨­å®š

4. **ãƒœãƒƒãƒˆæ¤œå‡º**
   - Crawl4AIã®anti_detectionæ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
   - ãã‚Œã§ã‚‚ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã‚‹å ´åˆã¯ãƒ—ãƒ­ã‚­ã‚·æ¤œè¨

---

## å‚è€ƒãƒªãƒ³ã‚¯

- Crawl4AI GitHub: https://github.com/unclecode/crawl4ai
- Crawl4AI Docs: https://docs.crawl4ai.com/
- LPã‚¢ãƒ¼ã‚«ã‚¤ãƒ–: https://rdlp.jp/lp-archive/
